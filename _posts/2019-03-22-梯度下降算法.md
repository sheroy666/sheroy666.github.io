---
layout:     post
title:      梯度下降算法
subtitle:   线性回归
date:       2019-03-22
author:     Sheroy
header-img: img/post-bg-rwd2.jpg
catalog: true
tags:
    - gradient descent
---


## 单变量线性回归---梯度下降算法 ## 

> 可以最小化多参数的代价函数


- 理解：从山上的某一点出发，在这点环顾四周找到最快下山的方向，到了下一个点继续找最快下山的方向，知道找出局部的最低点（最小值）。
- ：=    ----赋值运算符，即把右边表达式的值赋给左边的变量
- alpha ---学习率learning rate ---控制下山的步子有多大，越大则迈大步子下山，越小则迈小步子下山
-  两个参数必须同时更新，右下角是不对的，因为在算temp1时，0参数已经不是原来的值了，变成了新的值，这是不对的。应该按左边的方法来。



## 附页 ##
- 一元函数--求导---函数的变化率--逐渐增大还是减小 函数y=f(x)在某一点处沿x轴正方向的变化率
- 二元函数--求偏导--对于某个变量来说的函数变化率（另一变量为常数）--逐渐增大或减小 函数y=f(x1,x2,…,xn)在某一点处沿某一坐标轴（x1,x2,…,xn）正方向的变化率

> 在前面导 数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当我们讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。 
　- 通俗的解释是： 
　我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 
　
　- 梯度的提出只为回答一个问题： 
　函数在变量空间的某一点处，沿着哪一个方向有最大的变化率？ 
　- 梯度定义如下： 
　函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 
　这里注意三点： 
　1）梯度是一个向量，即有方向有大小； 
　2）梯度的方向是最大方向导数的方向； 
　3）梯度的值是最大方向导数的值。 

　提问：导数与偏导数与方向导数是向量么？ 
　向量的定义是有方向（direction）有大小（magnitude）的量。 
　从前面的定义可以这样看出，偏导数和方向导数表达的是函数在某一点沿某一方向的变化率，也是具有方向和大小的。因此从这个角度来理解，我们也可以把偏导数和方向导数看作是一个向量，向量的方向就是变化率的方向，向量的模，就是变化率的大小。 
　那么沿着这样一种思路，就可以如下理解梯度： 
　梯度即函数在某一点最大的方向导数，函数沿梯度方向函数有最大的变化率。 



- 学习率过小---收敛很慢
- 学习率过大----无法收敛（越过最低点了）
- 如果参数theta得到的函数值 J（theta）最开始就在局部最低点处，那么将直接收敛，即参数不变，因为导数为0。

- 随着越来越接近最低点，导数值derivative会自动越来越小（越来越缓），所以不必缩小alpha值，导数会使得步长自动越来越小。

> 针对线性回归中平方误差函数的梯度下降法：

- 平方误差函数---凸函数问题---只有全局最优值，没有局部最低点

> 批梯度下降法batch gradient descent：

- 每一次更新都是所有数据集都参与---求平均误差

- 正规方程法求解代价函数最小值









